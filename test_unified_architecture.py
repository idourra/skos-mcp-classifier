#!/usr/bin/env python3
"""
ğŸ§ª TEST UNIFIED ARCHITECTURE - Pruebas de la Arquitectura Unificada
================================================================
Valida el funcionamiento completo del sistema unificado:
- Data Gateway
- Processing Pipeline  
- Output Manager
- File Manager
- API Unificada
"""

import asyncio
import json
import sys
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Imports de la arquitectura unificada
try:
    from core.data_gateway import (
        DataRequest, DataSource, DataType, InputFormat, ProductInput,
        data_gateway, process_product
    )
    from core.processing_pipeline import (
        processing_pipeline, process_product_request
    )
    from core.output_manager import (
        OutputRequest, OutputMetadata, OutputType, OutputFormat,
        DeliveryMethod, OutputDestination, output_manager
    )
    from core.file_manager import (
        file_manager, FileType, FileFormat, store_taxonomy_file,
        export_classification_results
    )
    
    print("âœ… Todos los mÃ³dulos de la arquitectura unificada importados correctamente")
    
except ImportError as e:
    print(f"âŒ Error importando mÃ³dulos: {e}")
    print("ğŸ’¡ AsegÃºrate de que todos los archivos core/* existan y estÃ©n correctos")
    sys.exit(1)

class UnifiedArchitectureTest:
    """Clase principal para pruebas de la arquitectura unificada"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = datetime.now()
    
    async def run_all_tests(self):
        """Ejecutar todas las pruebas"""
        print("ğŸ§ª INICIANDO PRUEBAS DE ARQUITECTURA UNIFICADA")
        print("=" * 60)
        
        tests = [
            ("Data Gateway", self.test_data_gateway),
            ("Output Manager", self.test_output_manager), 
            ("File Manager", self.test_file_manager),
            ("Processing Pipeline", self.test_processing_pipeline),
            ("IntegraciÃ³n Completa", self.test_full_integration)
        ]
        
        for test_name, test_func in tests:
            print(f"\nğŸ”¬ Ejecutando: {test_name}")
            print("-" * 40)
            
            try:
                result = await test_func()
                self.test_results.append({
                    'test': test_name,
                    'success': result,
                    'timestamp': datetime.now().isoformat()
                })
                
                if result:
                    print(f"âœ… {test_name}: EXITOSO")
                else:
                    print(f"âŒ {test_name}: FALLIDO")
                    
            except Exception as e:
                print(f"ğŸ’¥ {test_name}: ERROR - {str(e)}")
                self.test_results.append({
                    'test': test_name,
                    'success': False,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
        
        await self.generate_test_report()
    
    async def test_data_gateway(self) -> bool:
        """Probar Data Gateway"""
        try:
            print("  ğŸ”¹ Probando ingesta de producto individual...")
            
            # Crear request de producto
            product_request = DataRequest(
                source=DataSource(
                    name="Test Product",
                    type=DataType.PRODUCT,
                    format=InputFormat.JSON
                ),
                data=ProductInput(
                    text="yogur griego natural 0% grasa",
                    product_id="TEST-001"
                )
            )
            
            # Procesar a travÃ©s del gateway
            result = await data_gateway.process_request(product_request)
            
            if result.status == "processed":
                print(f"    âœ… Gateway procesÃ³ {result.data_processed} elemento(s)")
                print(f"    ğŸ“Š Siguiente etapa: {result.next_stage}")
                return True
            else:
                print(f"    âŒ Gateway fallÃ³: {result.status}")
                return False
                
        except Exception as e:
            print(f"    ğŸ’¥ Error en Data Gateway: {str(e)}")
            return False
    
    async def test_output_manager(self) -> bool:
        """Probar Output Manager"""
        try:
            print("  ğŸ”¹ Probando entrega de resultados...")
            
            # Datos de prueba
            test_data = {
                "product_id": "TEST-001",
                "search_text": "yogur griego natural",
                "prefLabel": "Yogur",
                "notation": "10.20.30",
                "level": 3,
                "score": 0.95,
                "taxonomy_used": {
                    "id": "test-taxonomy",
                    "name": "Test Taxonomy"
                }
            }
            
            # Crear request de output
            output_request = OutputRequest(
                metadata=OutputMetadata(
                    type=OutputType.CLASSIFICATION_RESPONSE,
                    format=OutputFormat.JSON,
                    destination=OutputDestination(
                        method=DeliveryMethod.HTTP_RESPONSE,
                        target=""
                    )
                ),
                data=test_data
            )
            
            # Entregar a travÃ©s del output manager
            delivery_result = await output_manager.deliver_output(output_request)
            
            if delivery_result.success:
                print(f"    âœ… Output entregado: {delivery_result.output_id}")
                print(f"    ğŸ“¦ TamaÃ±o: {delivery_result.response_size} bytes")
                return True
            else:
                print(f"    âŒ Output fallÃ³: {delivery_result.errors}")
                return False
                
        except Exception as e:
            print(f"    ğŸ’¥ Error en Output Manager: {str(e)}")
            return False
    
    async def test_file_manager(self) -> bool:
        """Probar File Manager"""
        try:
            print("  ğŸ”¹ Probando gestiÃ³n de archivos...")
            
            # Crear archivo de prueba
            test_data = {
                "test": "data",
                "timestamp": datetime.now().isoformat(),
                "products": [
                    {"text": "producto 1", "id": "P1"},
                    {"text": "producto 2", "id": "P2"}
                ]
            }
            
            # Almacenar archivo
            file_metadata = await file_manager.store_file(
                content=json.dumps(test_data, indent=2),
                original_name="test_data.json",
                file_type=FileType.JSON_INPUT,
                file_format=FileFormat.JSON
            )
            
            print(f"    âœ… Archivo almacenado: {file_metadata.file_id}")
            print(f"    ğŸ“ Ruta: {file_metadata.relative_path}")
            
            # Procesar archivo
            operation = await file_manager.process_file(file_metadata.file_id)
            
            if operation.success:
                print(f"    âœ… Archivo procesado: {operation.operation_id}")
                processed_count = operation.result.get('records_count', 0)
                print(f"    ğŸ“Š Registros procesados: {processed_count}")
                return True
            else:
                print(f"    âŒ Procesamiento fallÃ³: {operation.errors}")
                return False
                
        except Exception as e:
            print(f"    ğŸ’¥ Error en File Manager: {str(e)}")
            return False
    
    async def test_processing_pipeline(self) -> bool:
        """Probar Processing Pipeline"""
        try:
            print("  ğŸ”¹ Probando pipeline de procesamiento...")
            
            # Simular procesamiento de producto (sin clasificaciÃ³n real)
            # ya que necesitarÃ­amos el sistema completo funcionando
            
            # Por ahora verificamos que el pipeline se inicialice correctamente
            stats = processing_pipeline.get_stats()
            
            print(f"    âœ… Pipeline inicializado")
            print(f"    ğŸ“Š Total procesados: {stats['total_processed']}")
            print(f"    âš¡ Tasa de Ã©xito: {stats['success_rate_percent']}%")
            
            # Verificar que los procesadores estÃ©n registrados
            stage_count = len(processing_pipeline.stage_processors)
            print(f"    ğŸ”§ Procesadores registrados: {stage_count}")
            
            return stage_count > 0
                
        except Exception as e:
            print(f"    ğŸ’¥ Error en Processing Pipeline: {str(e)}")
            return False
    
    async def test_full_integration(self) -> bool:
        """Probar integraciÃ³n completa"""
        try:
            print("  ğŸ”¹ Probando integraciÃ³n completa del sistema...")
            
            # Test 1: Verificar que todos los componentes estÃ©n disponibles
            components = {
                'data_gateway': data_gateway,
                'output_manager': output_manager,
                'file_manager': file_manager,
                'processing_pipeline': processing_pipeline
            }
            
            print("    ğŸ” Verificando componentes:")
            for name, component in components.items():
                if component:
                    print(f"      âœ… {name}: Disponible")
                else:
                    print(f"      âŒ {name}: No disponible")
                    return False
            
            # Test 2: Verificar mÃ©todos principales
            methods_test = {
                'gateway.process_request': hasattr(data_gateway, 'process_request'),
                'output_manager.deliver_output': hasattr(output_manager, 'deliver_output'),
                'file_manager.store_file': hasattr(file_manager, 'store_file'),
                'pipeline.process': hasattr(processing_pipeline, 'process')
            }
            
            print("    ğŸ” Verificando mÃ©todos:")
            for method, available in methods_test.items():
                if available:
                    print(f"      âœ… {method}: Disponible")
                else:
                    print(f"      âŒ {method}: No disponible")
                    return False
            
            # Test 3: Verificar estadÃ­sticas
            try:
                gateway_stats = {"message": "Gateway stats not implemented"}  # Placeholder
                output_stats = output_manager.get_stats()
                file_stats = file_manager.get_stats()
                pipeline_stats = processing_pipeline.get_stats()
                
                print("    ğŸ“Š EstadÃ­sticas del sistema:")
                print(f"      ğŸ“¤ Output: {output_stats['total_outputs']} entregas")
                print(f"      ğŸ“ Files: {file_stats['total_files']} archivos")
                print(f"      âš™ï¸ Pipeline: {pipeline_stats['total_processed']} procesados")
                
            except Exception as e:
                print(f"    âš ï¸ Error obteniendo estadÃ­sticas: {str(e)}")
                # No es crÃ­tico para la integraciÃ³n
            
            print("    âœ… IntegraciÃ³n completa verificada")
            return True
            
        except Exception as e:
            print(f"    ğŸ’¥ Error en integraciÃ³n completa: {str(e)}")
            return False
    
    async def generate_test_report(self):
        """Generar reporte de pruebas"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ REPORTE DE PRUEBAS")
        print("=" * 60)
        
        total_tests = len(self.test_results)
        successful_tests = len([r for r in self.test_results if r['success']])
        failed_tests = total_tests - successful_tests
        
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        
        print(f"ğŸ“Š Resumen:")
        print(f"  Total de pruebas: {total_tests}")
        print(f"  Exitosas: {successful_tests}")
        print(f"  Fallidas: {failed_tests}")
        print(f"  Tasa de Ã©xito: {success_rate:.1f}%")
        
        print(f"\nâ±ï¸ Tiempo total: {(datetime.now() - self.start_time).total_seconds():.2f} segundos")
        
        # Detalles de pruebas fallidas
        failed_details = [r for r in self.test_results if not r['success']]
        if failed_details:
            print(f"\nâŒ Pruebas fallidas:")
            for failure in failed_details:
                print(f"  â€¢ {failure['test']}")
                if 'error' in failure:
                    print(f"    Error: {failure['error']}")
        
        # Generar archivo de reporte
        try:
            report_data = {
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_tests': total_tests,
                    'successful': successful_tests,
                    'failed': failed_tests,
                    'success_rate_percent': round(success_rate, 1)
                },
                'test_results': self.test_results,
                'execution_time_seconds': (datetime.now() - self.start_time).total_seconds()
            }
            
            # Crear directorio de reportes si no existe
            reports_dir = Path("test_reports")
            reports_dir.mkdir(exist_ok=True)
            
            # Guardar reporte
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = reports_dir / f"unified_architecture_test_{timestamp}.json"
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ“„ Reporte guardado en: {report_path}")
            
        except Exception as e:
            print(f"\nâš ï¸ No se pudo guardar el reporte: {str(e)}")
        
        # ConclusiÃ³n
        if success_rate >= 80:
            print(f"\nğŸ‰ ARQUITECTURA UNIFICADA: FUNCIONANDO CORRECTAMENTE")
            print("âœ… El sistema estÃ¡ listo para uso en producciÃ³n")
        elif success_rate >= 60:
            print(f"\nâš ï¸ ARQUITECTURA UNIFICADA: FUNCIONAMIENTO PARCIAL")
            print("ğŸ”§ Revisa las pruebas fallidas antes de usar en producciÃ³n")
        else:
            print(f"\nâŒ ARQUITECTURA UNIFICADA: REQUIERE CORRECCIONES")
            print("ğŸš¨ No usar en producciÃ³n hasta corregir los problemas")

async def main():
    """FunciÃ³n principal"""
    print("ğŸŒŸ UNIFIED SKOS ARCHITECTURE - TEST SUITE")
    print(f"â° Iniciado el {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    test_suite = UnifiedArchitectureTest()
    await test_suite.run_all_tests()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ Pruebas interrumpidas por el usuario")
    except Exception as e:
        print(f"\nğŸ’¥ Error crÃ­tico en las pruebas: {str(e)}")
        sys.exit(1)